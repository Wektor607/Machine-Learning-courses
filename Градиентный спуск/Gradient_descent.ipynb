{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJCWS8t0yPZT",
        "outputId": "37ec1324-ff0f-41f4-a1b6-c7579df3bcd8"
      },
      "source": [
        "#Реализация градиентного спуска\n",
        "\n",
        "#First method\n",
        "import torch\n",
        "\n",
        "w = torch.tensor([[5., 10.], [1., 2.]], requires_grad=True)\n",
        "alpha = 0.001\n",
        "\n",
        "for i in range(500):\n",
        "    function = (torch.log(torch.log(w + 7))).prod()\n",
        "    function.backward()\n",
        "    w.data = w.data - alpha * w.grad\n",
        "    w.grad.zero_() # на каждом шаге обнуляем градиент, так как в PyTorch он накапливается\n",
        "print(w)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.9900, 9.9948],\n",
            "        [0.9775, 1.9825]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeL6QW7H9bDx",
        "outputId": "36b6ea82-c43b-447c-8dac-adb608c0a068"
      },
      "source": [
        "#Second method\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "w = torch.tensor([[5., 10.], [1., 2.]], requires_grad=True)\n",
        "alpha = 0.001\n",
        "optimizer =  torch.optim.SGD([w], lr = alpha) # SGD - стахостистический градиентный спуск, lr - градиентный шаг \n",
        "\n",
        "def function_double_log(x):\n",
        "    return (torch.log(torch.log(w + 7))).prod()\n",
        "\n",
        "def make_gradient_step(function, x):\n",
        "    function_result = function(x)\n",
        "    function_result.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "for i in range(500):\n",
        "    make_gradient_step(function_double_log, w)\n",
        "\n",
        "print(w) # Код для самопроверки\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.9900, 9.9948],\n",
            "        [0.9775, 1.9825]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}